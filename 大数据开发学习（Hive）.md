# Hadoop学习指南（Hive篇）
-------

## 一 Hive介绍

- CLI(command line interface)
- JDBC/ODBC
- Thrift server
- web gui
- metastore：
存储hive的元数据
- driver(complier,optimizer和executor)：
将Hive QL语句进行解析、编译优化，生成执行计划，然后调用底层的计算框架（诸如mapreduce,tez等）。

服务器端组件和客户端组件。
![](static/Screen Shot 2017-03-03 at 11.02.59.png)

## 二 Hive安装
此处演示的hive安装环境为：

- CentOS 6.5 64位
- Hive 2.1.1
- Java 1.8




## 三 关于HiveQL
HiveQL虽然基于SQL，但是并不严格遵循SQL-92的标准。HiveQL提供SQL没有的扩展，包括：

## 四 Hive的使用


- 创建数据库

```
-- 创建hello_world数据库
create database hello_world;  
```
- 查看所有数据库

```
show databases;
```

- 查看所有表

```
show tables;
```

- 创建内部表

```
-- 创建hello_world_inner
create table hello_world_inner
(
    id bigint, 
    account string, 
    name string,
    age int
)
row format delimited fields terminated by '\t';
```

- 创建分区表

```
create table hello_world_parti
(
	id bigint,
	name string
)
partitioned by (dt string, country string)
;
```
- 展示表分区

```
show partition hello_world_parti;
```

- 创建外部表

```

```

注外部表和hive内的表区别：

1、在导入数据到外部表，数据并没有移动到自己的数据仓库目录下，也就是说外部表中的数据并不是由它自己来管理的！而表则不一样；

2、在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！

- 更改表名称

```
alter table hello_world_parti to hello_world2_parti;
```

- 修改表结构

```
-- 新增列
alter table fact_jinrong_submit add columns(amount string, period string);
```

- 对表中的某一列进行修改，包括列的名称/列的数据类型/列的位置/列的注释

```
ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type[COMMENT col_comment] [FIRST|AFTER column_name]
这个命令可以允许用户修改一个列的名称、数据类型、注释或者位置
create table test_col_change (a int,b int, c int);

修改列的名称，后面一定要加上数据类型：
ALTER TABLE test_col_change CHANGE a a1 INT; 将 a 列的名字改为 a1.

ALTER TABLE test_col_change CHANGE a a1 STRING AFTER b; 将 a 列的名字改为 a1，a 列的数据类型改为string，并将它放置在列 b 之后。新的表结构为： b int, a1 string, c int.

ALTER TABLE test_col_change CHANGE b b1 INT FIRST; 会将 b 列的名字修改为b1, 并将它放在第一列。新表的结构为： b1 int, a string, c int.

注意：对列的改变只会修改Hive 的元数据，而不会改变实际数据。用户应该确定保证元数据定义和实际数据结构的一致性。
```


- 导入数据

```
load data local inpath '/home/deploy/user_info.txt' into table user_info;
```
- 导出数据

```

```

## 四 导入数据的几种方式
比如有一张测试表：

```
create table hello
(
id int,
name string,
message string
)
partitioned by (
dt string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
;
```

- 从本地文件系统中导入数据到hive表

```
load data local inpath 'data.txt' into table hello;
```

- 从HDFS上导入数据到hive表
- 从别的表中查询出相应的数据并导入到hive表中
- 创建表时从别的表查到数据并插入的所创建的表中



## 四 Hive Architecture

![](static/Screen Shot 2017-01-05 at 12.14.07.png)

引自官网，务必仔细阅读：
>Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table's location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).

一个完成的hive查询过程如上所述。上次过程，也展示了hive的主要组件，以及同hadoop的接口。这些重要的组件是：

- UI：用户向系统提交查询及其他操作。
- Driver：接受用户发来的查询等操作。
- Compiler：解析查询，包括：
- Metastore：存储所有的
- Execution Engine：执行



## 五 Hive Data Model
Hive的数据被组织成：

- tables
- partitions
- buckets

## 六 Metastore
这是hive



## 五 常见问题

### 1. 多次初始化
可能解法:在终端输入`hive`命令时，hive的CLI会自动加载一个配置文件：

/home/deploy/apache-hive-2.1.1-bin/conf/hive-log4j2.properties

文件内容（可以略过）：

```
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

status = INFO
name = HiveLog4j2
packages = org.apache.hadoop.hive.ql.log

# list of properties
property.hive.log.level = INFO
property.hive.root.logger = DRFA
property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
property.hive.log.file = hive.log
property.hive.perflogger.log.level = INFO

# list of all appenders
appenders = console, DRFA

# console appender
appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n

# daily rolling file appender
appender.DRFA.type = RollingRandomAccessFile
appender.DRFA.name = DRFA
appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}
# Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session
appender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}
appender.DRFA.layout.type = PatternLayout
appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
appender.DRFA.policies.type = Policies
appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
appender.DRFA.policies.time.interval = 1
appender.DRFA.policies.time.modulate = true
appender.DRFA.strategy.type = DefaultRolloverStrategy
appender.DRFA.strategy.max = 30

# list of all loggers
loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger

logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
logger.NIOServerCnxn.level = WARN

logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
logger.ClientCnxnSocketNIO.level = WARN

logger.DataNucleus.name = DataNucleus
logger.DataNucleus.level = ERROR
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```
可以删除已存在的该文件，然后再次初始化一次形成新文件。


### 2. 指定mysql，无法初始化
一般是用户权限问题，以及错误地指定了mysql的连接IP和端口。


### 3. Could not obtain block


### 4. 修改分区字段类型
常见的一个场景是Hive里面一个带分区的表，原来是int类型的字段，后来发现数据超过了int的最大值，要改成bigint。或者是bigint要改string或decimal。无论如何，对于带分区的表，要改列类型，有一个坑：
 
如果使用`alter table t change column oldcol newcol bigint`，即把int类型的oldcol改为bigint类型的newcol
 
这个时候，去读数据，应该还是NULL的。
 
这是因为每个分区Hive还会存一份元数据，于是两种解决方案：
 
- alter table t change column oldcol newcol bigint cascade;
 
- alter table t change column oldcol newcol bigint, alter table t partition(...) change column oldcol newcol bigint;

同时，在修改表字段类型时，会遇到各种限制：
![](static/20160805174521925.png)

特别是string转int，hive不让转。如果某个字段一定是int值类型，但是不小心存成了string，这个时候可以先取消hive内置的限制。在hive CLI中执行：`set hive.metastore.disallow.incompatible.col.type.changes=false;`

执行string to int的sql,例如：将string类型的unix时间戳（createdtime字段）转成int，则`alter table fact_jinrong_submit change column createdtime createdtime int cascade;`

最后，一定要注意恢复该限制：`set hive.metastore.disallow.incompatible.col.type.changes=true;`

## 六 参考资料

- [大数据时代的技术hive：hive介绍](http://www.cnblogs.com/sharpxiajun/archive/2013/06/02/3114180.html)
- [CentOS6.5安装hive-2.1.0](http://blog.csdn.net/u012592062/article/details/53022447)
- [Hive官方文档翻译——Hive Tutorial（上）（Hive 入门指导）](http://blog.csdn.net/u011812294/article/details/53610210)
- [Hive学习系列(一)什么是Hive及Hive的架构](http://xianglp.iteye.com/blog/2338124)
- [Hive学习系列(二)Hive的查询流程详解](http://xianglp.iteye.com/blog/2338806)
- [Hive几种数据导入方式](https://www.iteblog.com/archives/949)
- [Hive的数据存储模式](https://www.iteblog.com/archives/866)
- [Hive数据类型转换](https://www.iteblog.com/archives/892.html)
- [Hive的几种内置服务](https://www.iteblog.com/archives/957.html)
- [Hadoop Hive概念学习系列之hive里的扩展接口（CLI、Beeline、JDBC）](http://www.cnblogs.com/zlslch/p/6105571.html)